---
title: "Deploying a High Availability Teleport Cluster"
description: "Deploying a High Availability Teleport Cluster"
---

A Teleport deployment is a distributed system, and you can set up your Teleport
deployment to ensure that it can survive faults in one part of the system so
users can continue to access infrastructure. In this guide, we will explain the
components of a high availability Teleport deployment, including:

- The Teleport components you should deploy to ensure high availability.
- How to configure Teleport components.
- The infrastructure components besides Teleport you should deploy. 

For parts of your infrastructure outside Teleport's binaries, this guide does
not recommend individual products or tools, but is more of a checklist to ensure
that you have planned a complete HA Teleport deployment.

You can find complete reference deployments, including recommendations for all
required components, in the following sections:

- [HA Teleport Deployments on Kubernetes with Helm](helm-deployments.mdx)
- [How to Deploy to your Cloud](deployments.mdx)

## Overview

A high-availability Teleport cluster runs is a group of `teleport` processes,
each of which runs the Auth Service and Proxy Service, plus the infrastructure
required to support them. This includes:

- A Layer Four load balancer to direct traffic from users and services to an
  available `teleport` process.
- A storage backend for Auth Service state that all `teleport` processes can
  access. This requires permissions for `teleport` instnaces to manage
  records within the storage backend.
- A persistent object store for session recordings, which the Auth Service must
  have access to. This requires permissions for `teleport` instnaces to manage
  objects within the store.
- A DNS service that {/*what?*/} can use for completing the Let's Encrypt DNS-01
  challenge. This requires permissions for {/* what?*/} to manage DNS records.
- DNS A records for the Proxy Service's web frontend.
- An automated system for obtaining TLS credentials from Let's Encrypt, renewing
  the TLS credentials, and provisioning `teleport` processes with them.

{/* TODO: include a diagram--do this after filling in the rest of the details*/}

Once your Teleport deployment is up and running, you can add resources by
launching the Teleport Application Service, Database Service, Kubernetes
Service, and more. These services are outside the scope of this guide. 

## Layer Four load balancer

The load balancer forwoards traffic from users and services to an available
Teleport instance. The load balancer must not terminate TLS, and must simply
forward the traffic it receives. As a result, only Layer Four load balancers are
appropriate for a high-availability teleport deployment.

Configure the load balancer to forward traffic from the following ports on the
load balancer to the corresponding port on an available Teleport instance:

| Port | Description |
| - | - |
| 3021 | Port used by Teleport Proxy Service instances to dial agents in Proxy Peering mode. |
| 3022 | SSH port. This is Teleport's equivalent of port `#22` for SSH. |
| 3023 | SSH port clients connect to. The Proxy Service will forward this connection to port `#3022` on the destination Node. |
| 3024 | SSH port used to create reverse SSH tunnels from behind-firewall environments. |
| 3025 | TLS port used by the Auth Service to serve its API to other Nodes in a cluster. |
| 3080 or 443 | HTTPS connection to authenticate `tsh` users into the cluster. The same connection is used to serve a Web UI. |
| 3026 | HTTPS Kubernetes proxy |
| 3027 | Kubernetes Service |
| 3028 | Desktop Service |
| 3036 | MySQL port |
| 5432 | Postgres port |

## Auth Service backend

The Teleport Auth Service uses a backend to store audit events, Teleport cluster
state, and session recordings. Single-instance Auth Service deployments use a
local sqlite database for storage. For high-availability deployments, you must
deploy the backend separately from your cluster of Teleport instances. 

### Cluster state and audit events

The Teleport Auth Service stores cluster state (such as dynamic configuration
resources) and audit events as key/value pairs. 

The Auth Service supports the following backends for cluster state and audit
events:

- AWS DynamoDB
- Google Cloud Firestore
- etcd

For AWS DynamoDB and Google Cloud Firestore, your Teleport configuration (which
we will describe in more detail in the [Configuration](#configuration) section)
names a table or collection where Teleport stores cluster state and audit
events. For etcd, Teleport uses namespaces within item keys to identify audit
events and cluster state data.

In AWS DynamoDB and Google Cloud Firestore, the Teleport Auth Service manages
the creation of any required tables or collections itself, and does not require
them to exist in advance.

<Admonition title="Required permissions">

In your cloud provider's RBAC solution, your Teleport instances need permissions
to read from and write to your chosen key/value store, as well as to create
tables and collections (if your key/value store supports them).

</Admonition>

### Session recordings

High-availability Teleport deployments use an object storage service for session
recordings. The Teleport Auth Service supports two object storage services:

- Amazon S3 
- Google Cloud Storage

In your Teleport configuration (described in the [Configuration](#configuration)
section), you must name a GCS or S3 bucket to use for managing session
recordings. The Teleport Auth Service creates this bucket, so to prevent
unexpected behavior, you should not create it in advance. 

<Admonition title="Required permissions">

In your cloud provider's RBAC solution, your Teleport instances need
permissions to create, read from, and write to buckets.

</Admonition>

## DNS service

The Teleport Proxy Service must be reachable from both the Teleport Auth Service
and Teleport services like the Database Service and Kubernetes Service. For most
users, this means exposing at least one port within the Teleport Proxy Service
to the public internet, the HTTPS port `443`. 

For most users, we recommend running the HTTPS port using TLS credentials
provided by Let's Encrypt. This requires a registered domain name. 

## TLS credential provisioning 

HA Teleport deployments require an automated system to renew certificates from
Let's Encrypt and provision Teleport instances with them. 

When you are running a single instance of the Teleport Auth Service and Proxy
Service, you can configure this instance to fetch it's own credentials from
Let's Encrypt using the ALPN-01 ACME challenge, where Teleport demonstrates that
it controls the ALPN server at a particular domain.

For high-availability deployments, however, it is much more efficient to use a
separate system to request TLS credentials from Let's Encrypt and provision
Teleport instances with them. In this case, with multiple Teleport instances
behind a load balancer, you will need to use the ACME DNS-01 challenge to
demonstrate domain name ownership to Let's Encrypt. In this challenge, your TLS
credential provisioning system creates a DNS TXT record with a value expected
by Let's Encrypt.

The TLS credential provisioner we mentioned earlier needs permissions to manage
DNS records in order to satisfy Let's Encrypt's DNS-01 challenge. If you are
using cloud-managed solutions, you should use your cloud provider's RBAC system
(e.g., AWS IAM) to grant a role to the Proxy Service to manage DNS records. 

In the configuration we are demonstrating in this guide, each Teleport instance
expects TLS credentials for HTTPS to be available at the file paths
`/etc/teleport-tls/tls.key` (private key) and `/etc/teleport-tls/tls.crt`
(certificate).

## Teleport instances

Run the Teleport Auth Service and Proxy Service as a scalable group of compute
resources, for example, a Kubernetes `Deployment`  or AWS Auto Scaling group. 

### Open ports

Ensure that, on each Teleport instances, the following ports allow traffic from
the load balancer. If you do not plan to run a service associated with a given
port, you can plan to leave that port closed.

| Port | Description |
| - | - |
| 3021 | Port used by Teleport Proxy Service instances to dial agents in Proxy Peering mode. |
| 3022 | SSH port. This is Teleport's equivalent of port `#22` for SSH. |
| 3023 | SSH port clients connect to. The Proxy Service will forward this connection to port `#3022` on the destination Node. |
| 3024 | SSH port used to create reverse SSH tunnels from behind-firewall environments. |
| 3025 | TLS port used by the Auth Service to serve its API to other Nodes in a cluster. |
| 3080 or 443 | HTTPS connection to authenticate `tsh` users into the cluster. The same connection is used to serve a Web UI. |
| 3026 | HTTPS Kubernetes proxy |
| 3027 | Kubernetes Service |
| 3028 | Desktop Service |
| 3036 | MySQL port |
| 5432 | Postgres port |

*This is the same table of ports we used to configure the load balancer.*

### License file

If you are deploying Teleport Enterprise, you need to download a license file
and make it available to your Teleport instances.

To obtain your license file, visit the [Teleport customer
dashboard](https://dashboard.gravitational.com/web/login) and log in. Click
"DOWNLOAD LICENSE KEY". You will see your current Teleport Enterprise account
permissions and the option to download your license file:

![License File modal](../../../img/enterprise/license.png)

The license file must be available to each Teleport instance at
`/var/lib/teleport/license.pem`. 

### Configuration

Create a configuration file and provide it to each of your Teleport instances at
`/etc/teleport.yaml`. We will explain the required configuration fields for a
high-availability Teleport deployment below. These are the minimum requirements,
and when planning your high-availability deployment, you will want to follow a
more specific [deployment guide](introduction.mdx) for your environment. 

Here is an example high-availability configuration file configured for two
backends, AWS DynamoDB and Google Cloud Firestore:

<Tabs>
<TabItem label="AWS DynamoDB">

```yaml
    version: v1
    teleport:
      storage:
        type: dynamodb
        region: "us-east-1" # Or another region
        table_name: "my-kv-table" 
        audit_events_uri: ["dynamodb://my-audit-events-table", "stdout://"]
        continuous_backups: true
        audit_sessions_uri: "s3://my-session-recording-bucket"
     auth_service:
      enabled: true
      cluster_name: "mycluster.example.com"
      # Remove this if not using Teleport Enterprise
      license_file: "/var/lib/license/license.pem"
    proxy_service:
      public_addr: "mycluster.example.com:443"
      enabled: true
      https_keypairs:
      - key_file: /etc/teleport-tls/tls.key
        cert_file: /etc/teleport-tls/tls.crt
    ssh_service:
      enabled: false
```
</TabItem>
<TabItem label="Google cloud Firestore">

```yaml
    version: v1
    teleport:
      storage:
        type: firestore
        project_id: my-google-cloud-project
        collection_name: my-cluster-state-table
        credentials_path: /etc/teleport-secrets/gcp-credentials.json
        audit_events_uri: ['firestore://my-audit-events-table?projectID=my-google-cloud-project&credentialsPath=/etc/teleport-secrets/gcp-credentials.json', 'stdout://']
        audit_sessions_uri: "gs://session-recording-bucket?projectID=my-google-cloud-project&credentialsPath=/etc/teleport-secrets/gcp-credentials.json"
     auth_service:
      enabled: true
      cluster_name: "mycluster.example.com"
      # Remove this if not using Teleport Enterprise
      license_file: "/var/lib/license/license.pem"
    proxy_service:
      public_addr: "mycluster.example.com:443"
      enabled: true
      https_keypairs:
      - key_file: /etc/teleport-tls/tls.key
        cert_file: /etc/teleport-tls/tls.crt
    ssh_service:
      enabled: false
```
</TabItem>
<TabItem label="etcd">
```yaml
version: v1
teleport:
  storage:
    type: etcd
    peers: ["https://172.17.0.1:4001", "https://172.17.0.2:4001"]
    tls_cert_file: /var/lib/teleport/etcd-cert.pem
    tls_key_file: /var/lib/teleport/etcd-key.pem
    tls_ca_file: /var/lib/teleport/etcd-ca.pem
    username: username
    password_file: /mnt/secrets/etcd-pass
    prefix: /teleport/
    insecure: false
    etcd_max_client_msg_size_bytes: 15728640
    audit_sessions_uri: "s3://my-session-recording-bucket"
  auth_service:
    enabled: true
    cluster_name: "mycluster.example.com"
    # Remove this if not using Teleport Enterprise
    license_file: "/var/lib/license/license.pem"
  proxy_service:
    public_addr: "mycluster.example.com:443"
    enabled: true
    https_keypairs:
    - key_file: /etc/teleport-tls/tls.key
      cert_file: /etc/teleport-tls/tls.crt
  ssh_service:
    enabled: false
```
</TabItem>
</Tabs>

This configuration uses the `v1` version to disable [TLS
multiplexing](../management/operations/tls-routing.mdx), which is required when
running Teleport with a load balancer in high-availability mode. 

The `storage` section configures the Auth Service backend. The
`audit_sessions_uri` names the GCS or S3 bucket to use for session recordings,
and the remaining fields in `storage` are specific to each backend. Consult our
[Backends Reference](../reference/backends.mdx) for the values you should use.

In the `auth_service` section, we have enabled the Teleport Auth Service and
instructed it to find an Enterprise license file at
`/var/lib/license/license.pem`.

In the `proxy_service` section, we have enabled the Teleport Proxy Service and
instructed it to find its TLS credentials in the `/etc/teleport-tls` directory.

In our example configurations, we have disabled the Teleport SSH Service by
setting `ssh_service` to `false`. This is suitable for Kubernetes deployments of
Teleport, where the `teleport` pod should not have direct access to the
underlying node. If deploying Teleport on a virtual machine, remove this line to
enable the SSH Service and enable secure access to the host.

## Next steps

Now that you know the general principles behind an HA Teleport deployment, read
about how to implement your own deployment on Kubernetes or a cluster of VMs in
your cloud of choice:

- [HA Teleport Deployments on Kubernetes with Helm](helm-deployments.mdx)
- [How to Deploy to your Cloud](deployments.mdx)

Teleport supports a range of Auth Service backends, and you can plan the best
one for your use case in our [backends guide](../reference/backends.mdx).

You should also get familiar with how to ensure that your Teleport deployment is
performing as expected:

- [See our guidelines for scaling a Teleport
  cluster](../management/operations/scaling.mdx)
- [Read our guides to monitoring a Teleport
  cluster](../management/diagnostics.mdx)
