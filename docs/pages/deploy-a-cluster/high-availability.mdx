---
title: "Deploying a High Availability Teleport Cluster"
description: "Deploying a High Availability Teleport Cluster"
---

A Teleport deployment is a distributed system, and you can set up your Teleport
deployment to ensure that it can survive faults in one part of the system so
users can continue to access infrastructure. In this guide, we will explain the
components of a high availability Teleport deployment, including:

- The Teleport components you should deploy to ensure high availability.
- How to configure Teleport components.
- The infrastructure components besides Teleport you should deploy. 

For parts of your infrastructure outside Teleport's binaries, this guide does
not recommend individual products or tools, but is more of a checklist to ensure
that you have planned a complete HA Teleport deployment.

You can find complete reference deployments, including recommendations for all
required components, in the following sections:

- [HA Teleport Deployments on Kubernetes with Helm](helm-deployments.mdx)
- [How to Deploy to your Cloud](deployments.mdx)

## Overview

A high-availability Teleport cluster runs is a group of `teleport` processes,
each of which runs the Auth Service and Proxy Service, plus the infrastructure
required to support them. This includes:

- A Layer Four load balancer to direct traffic from users and services to an
  available `teleport` process.
- A storage backend for Auth Service state that all `teleport` processes can
  access. This requires permissions for `teleport` instnaces to manage
  records within the storage backend.
- A persistent object store for session recordings, which the Auth Service must
  have access to. This requires permissions for `teleport` instnaces to manage
  objects within the store.
- A DNS service that {/*what?*/} can use for completing the Let's Encrypt DNS-01
  challenge. This requires permissions for {/* what?*/} to manage DNS records.
- DNS A records for the Proxy Service's web frontend.
- An automated system for obtaining TLS credentials from Let's Encrypt, renewing
  the TLS credentials, and provisioning `teleport` processes with them.

{/* TODO: include the diagram--do this after filling in the rest of the
details*/}

Once your Teleport deployment is up and running, you can add resources by
launching the Teleport Application Service, Database Service, Kubernetes
Service, and more. These services are outside the scope of this guide. 

{/*TODO: For each of these, include a list of examples in a Notice*/}

## Layer Four load balancer

The load balancer forwoards traffic from users and services to an available
Teleport instance. The load balancer must not terminate TLS, and must simply
forward the traffic it receives. As a result, only Layer Four load balancers are
appropriate for a high-availability teleport deployment.

Configure the load balancer to forward traffic from the following ports on the
load balancer to the corresponding port on an available Teleport instance:

| Port | Description |
| - | - |
| 3021 | Port used by Teleport Proxy Service instances to dial agents in Proxy Peering mode. |
| 3022 | SSH port. This is Teleport's equivalent of port `#22` for SSH. |
| 3023 | SSH port clients connect to. The Proxy Service will forward this connection to port `#3022` on the destination Node. |
| 3024 | SSH port used to create reverse SSH tunnels from behind-firewall environments. |
| 3025 | TLS port used by the Auth Service to serve its API to other Nodes in a cluster. |
| 3080 or 443 | HTTPS connection to authenticate `tsh` users into the cluster. The same connection is used to serve a Web UI. |
| 3026 | HTTPS Kubernetes proxy |
| 3027 | Kubernetes Service |
| 3028 | Desktop Service |
| 3036 | MySQL port |
| 5432 | Postgres port |

## Auth Service backend

{/*TODO: research/outline the requirements*/}

The Teleport Auth Service needs access to its backend. If you are using
cloud-managed solutions, you should use your cloud provider's RBAC system (e.g.,
AWS IAM) to grant a role to the Auth Service to read data from and write data to
the backend and session store.

## Session recording storage

{/*TODO: research/outline the requirements*/}

The Teleport Auth Service needs access to its session recording store. If you
are using cloud-managed solutions, you should use your cloud provider's RBAC
system (e.g., AWS IAM) to grant a role to the Auth Service to read data from and
write data to the backend and session store.

## DNS service

The Teleport Proxy Service must be reachable from both the Teleport Auth Service
and Teleport services like the Database Service and Kubernetes Service. For most
users, this means exposing at least one port within the Teleport Proxy Service
to the public internet, the HTTPS port `443`. 

For most users, we recommend running the HTTPS port using TLS credentials
provided by Let's Encrypt. This requires a registered domain name. 

## TLS credential provisioning 

HA Teleport deployments require an automated system to renew certificates from
Let's Encrypt and provision Teleport instances with them. 

When you are running a single instance of the Teleport Auth Service and Proxy
Service, you can configure this instance to fetch it's own credentials from
Let's Encrypt using the ALPN-01 ACME challenge, where Teleport demonstrates that
it controls the ALPN server at a particular domain.

For high-availability deployments, however, it is much more efficient to use a
separate system to request TLS credentials from Let's Encrypt and provision
Teleport instances with them. In this case, with multiple Teleport instances
behind a load balancer, you will need to use the ACME DNS-01 challenge to
demonstrate domain name ownership to Let's Encrypt. In this challenge, your TLS
credential provisioning system creates a DNS TXT record with a value expected
by Let's Encrypt.

The TLS credential provisioner we mentioned earlier needs permissions to manage
DNS records in order to satisfy Let's Encrypt's DNS-01 challenge. If you are
using cloud-managed solutions, you should use your cloud provider's RBAC system
(e.g., AWS IAM) to grant a role to the Proxy Service to manage DNS records. 

In the configuration we are demonstrating in this guide, each Teleport instance
expects TLS credentials for HTTPS to be available at the file paths
`/etc/teleport-tls/tls.key` (private key) and `/etc/teleport-tls/tls.crt`
(certificate).

## Teleport instances

Run the Teleport Auth Service and Proxy Service as a scalable group of compute
resources, for example, a Kubernetes `Deployment`  or AWS Auto Scaling group. 

### Open ports

Ensure that, on each Teleport instances, the following ports allow traffic from
the load balancer. If you do not plan to run a service associated with a given
port, you can plan to leave that port closed.

| Port | Description |
| - | - |
| 3021 | Port used by Teleport Proxy Service instances to dial agents in Proxy Peering mode. |
| 3022 | SSH port. This is Teleport's equivalent of port `#22` for SSH. |
| 3023 | SSH port clients connect to. The Proxy Service will forward this connection to port `#3022` on the destination Node. |
| 3024 | SSH port used to create reverse SSH tunnels from behind-firewall environments. |
| 3025 | TLS port used by the Auth Service to serve its API to other Nodes in a cluster. |
| 3080 or 443 | HTTPS connection to authenticate `tsh` users into the cluster. The same connection is used to serve a Web UI. |
| 3026 | HTTPS Kubernetes proxy |
| 3027 | Kubernetes Service |
| 3028 | Desktop Service |
| 3036 | MySQL port |
| 5432 | Postgres port |

*This is the same table of ports we used to configure the load balancer.*

### Configuration

Ensure that the each Teleport instance contains a configuration file similar to
the following. We have included examples of using two persistent storage
solutions for the Auth Service backend, audit logs, and storing session
recordings:

<Tabs>
<TabItem label="AWS DynamoDB">

{/*TODO: double-check the table_name and uri fields*/} 

```yaml
    version: v1
    teleport:
      storage:
        type: dynamodb
        region: "us-east-1" # Or another region
        table_name: "my-table" 
        audit_events_uri: ["dynamodb://my-audit-events-table", "stdout://"]
        audit_sessions_uri: "s3://my-session-recording-bucket"
        continuous_backups: true
     auth_service:
      enabled: true
      cluster_name: "mycluster.example.com"
      # Remove this if not using Teleport Enterprise
      license_file: "/var/lib/license/license.pem"
    proxy_service:
      public_addr: "mycluster.example.com:443"
      enabled: true
      https_keypairs:
      - key_file: /etc/teleport-tls/tls.key
        cert_file: /etc/teleport-tls/tls.crt
    ssh_service:
      enabled: false
```
</TabItem>
<TabItem label="AWS DynamoDB">

```yaml
    version: v1
    teleport:
      storage:
        type: firestore
        project_id: my-google-cloud-project
        collection_name: my-cluster-state-table
        credentials_path: /etc/teleport-secrets/gcp-credentials.json
        audit_events_uri: ['firestore://my-audit-events-table?projectID=my-google-cloud-project&credentialsPath=/etc/teleport-secrets/gcp-credentials.json', 'stdout://']
        audit_sessions_uri: "gs://session-recording-bucket?projectID=my-google-cloud-project&credentialsPath=/etc/teleport-secrets/gcp-credentials.json"
     auth_service:
      enabled: true
      cluster_name: "mycluster.example.com"
      # Remove this if not using Teleport Enterprise
      license_file: "/var/lib/license/license.pem"
    proxy_service:
      public_addr: "mycluster.example.com:443"
      enabled: true
      https_keypairs:
      - key_file: /etc/teleport-tls/tls.key
        cert_file: /etc/teleport-tls/tls.crt
    ssh_service:
      enabled: false
```
</TabItem>
</Tabs>

{/*TODO: explain important aspects of the configuration above*/}

## Next steps

Now that you know the general principles behind an HA Teleport deployment, read
about how to implement your own deployment on Kubernetes or a cluster of VMs in
your cloud of choice:

- [HA Teleport Deployments on Kubernetes with Helm](helm-deployments.mdx)
- [How to Deploy to your Cloud](deployments.mdx)

Teleport supports a range of Auth Service backends, and you can plan the best
one for your use case in our [backends guide](../reference/backends.mdx).

You should also get familiar with how to ensure that your Teleport deployment is
performing as expected:

- [See our guidelines for scaling a Teleport
  cluster](../management/operations/scaling.mdx)
- [Read our guides to monitoring a Teleport
  cluster](../management/diagnostics.mdx)
