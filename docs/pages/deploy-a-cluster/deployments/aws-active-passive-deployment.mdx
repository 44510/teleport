---
title: "AWS Multi-Region Active-Passive Architecture Deployment"
description: "Deploying a high-availability Teleport cluster in two AWS regions."
---

For mission-critical Teleport use cases, you may use the architecture described in this guide to achieve 
an automatic failover across multiple AWS regions. This architecture keeps Teleport accessible with minimal 
disruption during the event of an entire cloud provider region outage. While this example is for AWS, the 
general architecture can apply to various cloud providers and self-hosted examples as well.

In this architecture, the Auth and Proxy components and their networking components run parallelly in two 
different cloud regions. These components utilize a global DynamoDB storage backend for the cluster state and audit 
logs, while a copied S3 bucket is used for session recordings. The Route53 DNS failover is the control point to switch 
between the active and passive clusters during the event of a regional outage.

## Overview

![Diagram showing this Teleport
architecture](../../../img/deploy-a-cluster/aws-multi-region-active-passive-ha-deployment.png)

### Key Features
- Two sets of Teleport Auth and Proxy components are deployed in separate regions. For this example, the us-west-1
  region Teleport cluster is the Active cluster, and the us-west-1 region Teleport cluster is the Passive cluster.
- All teleport cluster components are deployed as autoscaling groups in both regions.
- DynamoDB is used for cluster state and audit log storage.
  - Enable the us-west-1 DynamoDB table with global replication in the us-east-1 passive region to ensure cluster 
    state and audit logs are maintained in both regions.
  - [DynamoDB Global Replication](https://aws.amazon.com/blogs/aws/new-convert-your-single-region-amazon-dynamodb-tables-to-global-tables/)
-  S3 buckets are for session recording storage.
  - Enable cross-region replication on the S3 bucket to ensure objects are replicated between both the active and 
    passive regions.
  - [S3 Replication](https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html).
  - Make sure proper IAM permissions are in place for the successful replication of 
  [S3 buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/setting-repl-config-perm-overview.html).
- configure the Teleport cluster Route53 records to use the 
  [failover routing policy](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-passive) 
  with the active cluster loadbalancer as the primary record and the passive cluster loadbalancer as the secondary record.
  - Use health checks to determine Teleport cluster availability. Route53 will automatically send the traffic to the 
    passive cluster during the event of a regional outage and it will automatically routes the traffic back to the active 
    cluster once the Active cluster when health checks report a normal state.

### Advantages of this deployment architecture

- Having a cluster in multi-regions makes the clusters quickly available even during regional outages.
- Downtime came be minimized vs traditional disaster recovery from a backup.
- All required Teleport components can be provisioned within the AWS ecosystem.


### Disadvantages of this deployment architecture

- Long-term cost may be a prohibitive factor for some organizations and can increase total cost of ownership
  (TCO) throughout the system's lifetime cycle.
- Deploying and maintaining the added layer of regional components takes more engineering effort.
- More technically complex to deploy than a single region Teleport cluster or simple disaster recovery.

### Configuration

##Prerequisites

(!docs/pages/kubernetes-access/helm/includes/teleport-cluster-prereqs.mdx!)

## Step 1/6. Install Helm

(!docs/pages/kubernetes-access/helm/includes/teleport-cluster-install.mdx!)

## Step 2/6. Add the Teleport Helm chart repository

(!docs/pages/kubernetes-access/helm/includes/helm-repo-add.mdx!)
## Step 3/6. Set up Teleport cluster in Primary Region (In my example us-west-1 is Primary Region) region:
- [Running an HA Teleport cluster using AWS, EKS, Helm](https://goteleport.com/docs/deploy-a-cluster/helm-deployments/aws/)
- [Configure Single Sing-on] (https://goteleport.com/docs/access-controls/sso/) and test your access to cluster.

## Step 4/6. Setup DynamoDB & S3 Global replication to Secondary Region (In my example us-east-1 is the Secondary Region):
- Check the Key components section of this document for Global replication.

## Step 5/6. Set up Teleport cluster in Secondary Region same as Step 1/7:
- We will use same `aws-values.yaml` & `aws-issuer.yaml` config to install Teleport name space in Secondary Region (us-east-1) by changing the clusterName.
  So that it will use same backend as defined in Primary Region (us-west-1) for DynamoDB and S3 buckets. 
  Once the Teleport Cluster in Secondary Region (us-east-1) is ready, you can check the SSO access by logging in via Secondary region (us-east-1) UI/tsh
  Configuration. SSO login is working from both the Primary (us-west-1) and Secondary (us-east-1) region Teleport clusters.

## Step 6/6. Configure Proxy Peering:
In this deployment architecture, [Proxy Peering](../../architecture/proxy-peering.mdx) is used to restrict the number of connections made from 
resources to proxies in the Teleport Cluster.

This guide covers the necessary Proxy Peering settings for deploying an HA Teleport Cluster routing resource
traffic with GSLB. 

### Auth Service Proxy Peering configuration 

The Teleport Auth Service must be configured to use the `proxy_peering` tunnel strategy as shown in the example below:

```
auth_service:
 ...
 tunnel_strategy:
  type: proxy_peering
  agent_connection_count: 2
```
Reference the [Auth Server configuration](../../reference/config.mdx#auth-service) reference page 
for additional settings.

<Notice type="warning">

With DynamoDB, the kube operator supports <=2 pod replicas only per cluster. So, It is not recommended to enable `kube operato` on 
secondary region.

</Notice>


