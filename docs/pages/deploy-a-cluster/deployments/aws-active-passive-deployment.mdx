---
title: "AWS Multi-Region Active-Passive Architecture Deployment"
description: "Deploying a high-availability Teleport cluster in two AWS regions."
---

For mission-critical Teleport use cases, you may use the architecture described in this 
guide to achieve an automatic failover across multiple AWS regions.
This architecture keeps Teleport accessible with minimal disruption during the event of an
entire cloud provider region outage. While this example is for AWS, the general architecture
can apply to various cloud providers and self-hosted examples as well.

In this architecture, the Auth and Proxy components and their networking components run 
parallelly in two different cloud regions. These components utilize a global DynamoDB storage 
backend for the cluster state and audit logs, while a copied S3 bucket is used for session recordings.
The Route53 DNS failoveris the control point to switch between the active and passive clusters during
the event of a regional outage.

## Overview

### Key Features
- Two sets of Teleport Auth and Proxy components are deployed in separate regions. For this example, the us-west-1
  region Teleport cluster is the Active cluster, and the us-west-1 region Teleport cluster is the Passive cluster.
- All teleport cluster components are deployed as autoscaling groups in both regions.
- DynamoDB is used for cluster state and audit log storage.
  - Enable the us-west-1 DynamoDB table with global replication in the us-east-1 passive region to ensure cluster 
    state and audit logs are maintained in both regions.
  - [DynamoDB Global Replication](https://aws.amazon.com/blogs/aws/new-convert-your-single-region-amazon-dynamodb-tables-to-global-tables/)
-  S3 buckets are for session recording storage.
  - Enable cross-region replication on the S3 bucket to ensure objects are replicated between both the active and 
    passive regions.
  - [S3 Replication](https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html).
  - Make sure proper IAM permissions are in place for the successful replication of 
  [S3 buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/setting-repl-config-perm-overview.html).
- configure the Teleport cluster Route53 records to use the 
  [failover routing policy](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-passive) 
  with the active cluster loadbalancer as the primary record and the passive cluster loadbalancer as the secondary record.
  - Use health checks to determine Teleport cluster availability. Route53 will automatically send the traffic to the 
    passive cluster during the event of a regional outage and it will automatically routes the traffic back to the active 
    cluster once the Active cluster when health checks report a normal state.

### Advantages of this deployment architecture

- Having a cluster in multi-regions makes the clusters quickly available even during regional outages.
- Downtime came be minimized vs traditional disaster recovery from a backup.
- All required Teleport components can be provisioned within the AWS ecosystem.


### Disadvantages of this deployment architecture

- Long-term cost may be a prohibitive factor for some organizations and can increase total cost of ownership
  (TCO) throughout the system's lifetime cycle.
- Deploying and maintaining the added layer of regional components takes more engineering effort.
- More technically complex to deploy than a single region Teleport cluster or simple disaster recovery.

### Configuration

##Prerequisites

Our code requires Terraform 0.13+. You can [download Terraform here](https://www.terraform.io/downloads.html). We will assume that you have
`terraform` installed and available on your path.

```code
$ which terraform
/usr/local/bin/terraform
$ terraform version
Terraform v1.2.6
```
## Step 1/7. Set up Teleport cluster in us-west-1 region:
- [Running an HA Teleport cluster using AWS, EKS, Helm](https://goteleport.com/docs/deploy-a-cluster/helm-deployments/aws/)
- [Configure Single Sing-on] (https://goteleport.com/docs/access-controls/sso/) and test your access to cluster.

## Step 2/7. Setup DynamoDB & S3 Global replication to region us-east-1:
- Check the Key components section of this document for Global replication.

## Step 3/7. Set up Teleport cluster in us-east-1 region as per Step 1/7:
- We will use same `aws-values.yaml` & `aws-issuer.yaml` config to install Teleport name space on us-east-1 by changing the clusterName.
  So that it will use same backend as defined in us-west-1 region for DynamoDB and S3 buckets. 
  Once the Teleport us-east-1 cluster is ready, you can check the SSO access by logging in via us-east-1 UI/tsh
  confim SSO is working from both the us-west-1 and us-east-1 Teleport clusters.

## Step 3/7. Set up Teleport cluster in us-east-1 region as per Step 1/7:
- We will enable kube `operatorEnabled: true` in `aws-values.yaml` and update the Teleport us-west-1 cluster using helm update
  Again, confim SSO is working from both the us-west-1 and us-east-1 Teleport clusters.

<Notice type="warning">

With DynamoDB, the kube operator supports >2 pod replicas only per cluster. This restriction for all Teleport clusters with DynamoDB 
with or with out kube operator.
We support multiple auth replicas. The operator sidecar elects a leader with a Kubernetes lease
if both operators sidecars are not in the same Kubernetes cluster and namespace they can't elect a leader. 
Without leader election they will fight against each other and harm Teleport's availability
If we enable kube operator for some on two clusters at a time then Teleport conflicts among both clusters and results in no availabilty.
They can disable the operator on the backup region, then perform a pod rollout on the primary region. When the operator starts, 
it deletes the existing operator bot user, and re-creates all required resources.

</Notice>

![Diagram showing this Teleport
architecture](../../../img/deploy-a-cluster/aws-multi-region-active-passive-ha-deployment.png)


