---
title: Backup and Restore
description: How to back up and restore your Teleport cluster state.
---
This guide explains the components of your Teleport deployment that must be
backed up and lays out our recommended approach for performing backups.

(!docs/pages/includes/cloud/call-to-action.mdx!)

## What you should back up

### Teleport services
<Tabs>
<TabItem scope={["enterprise", "oss"]} label="Self-Hosted">

Teleport's Proxy Service and Nodes are stateless. For these components, only
`teleport.yaml` should be backed up.

The Auth Service is Teleport's brain, and depending on the backend should be
backed up regularly.

For example, a Teleport cluster running on AWS with DynamoDB must back up the
following data:

| What | Where ( Example AWS Customer ) |
| - | - |
| Local Users ( not SSO ) | DynamoDB |
| Certificate Authorities | DynamoDB |
| Trusted Clusters | DynamoDB |
| Connectors: SSO | DynamoDB / File System |
| RBAC | DynamoDB / File System |
| teleport.yaml | File System |
| teleport.service | File System |
| license.pem | File System |
| TLS key/certificate | File System / AWS Certificate Manager |
| Audit log | DynamoDB |
| Session recordings | S3 |

For this customer, we would recommend using [AWS best practices](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html) for backing up DynamoDB. If DynamoDB is used for
Teleport audit logs, logged events have a TTL of 1 year.

| Backend | Recommended backup strategy |
| - | - |
| Local Filesystem | Back up the `/var/lib/teleport/storage` directory and the output of `tctl get all --with-secrets`. |
| DynamoDB | [Follow AWS's guidelines for backup and restore](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html) |
| etcd | [Follow etcd's guidelines for disaster recovery](https://etcd.io/docs/v2/admin_guide) |
| Firestore | [Follow GCP's guidelines for automated backups](https://firebase.google.com/docs/database/backups) |

</TabItem>
<TabItem scope={["cloud"]} label="Teleport Cloud">

Teleport Cloud manages all Auth Service and Proxy Service backups.

While Teleport Nodes are stateless, you should ensure that you can restore their
configuration files.

</TabItem>
</Tabs>

### Teleport resources

Teleport uses YAML resources for roles, Trusted Clusters, local users, and authentication connectors.
These could be created via `tctl` or the Web UI.

You should back up your dynamic resource configurations to ensure that you can restore them in case of an outage.

## Our recommended backup practice

If you're running Teleport at scale, your teams need to have an automated way to restore Teleport. At a high level, this is our recommended approach:

<Tabs>
<TabItem scope={["enterprise", "oss"]} label="Self-Hosted">

- Persist and back up your backend.
- Share that backend among Auth Service instances.
- Store your dynamic resource configurations as discrete files in a git
  repository.
- Have your continuous integration system run `tctl create -f *.yaml` from the
  git repository. The `-f` flag instructs `tctl create` not to return an error
  if a resource already exists, so this command can be run regularly.

</TabItem>
<TabItem scope={["cloud"]} label="Teleport Cloud">

- Store your dynamic resource configurations as discrete files in a git
  repository.
- Have your continuous integration system run `tctl create -f *.yaml` from the
  git repository. The `-f` flag instructs `tctl create` not to return an error
  if a resource already exists, so this command can be run regularly.

</TabItem>
</Tabs>

## Migrating backends
<Tabs>
<TabItem scope={["enterprise"]} label="Teleport Enterprise">

As of version v4.1, you can now quickly export a collection of resources from
Teleport. This feature was designed to help customers migrate from local storage
to etcd.

Using `tctl get all --with-secrets` will retrieve the below items:

- Users
- Certificate Authorities
- Trusted Clusters
- Connectors:
  - GitHub
  - SAML
  - OIDC
- Roles

When migrating backends, you should back up your Auth Service's
`data_dir/storage` directly.

### Example of backing up and restoring a cluster

```code
# Log in to your cluster with tsh so you can use tctl from your local machine.
# You can also run tctl on your Auth Service host without running "tsh login"
# first.
$ tsh login --proxy=teleport.example.com --user=myuser
# Export dynamic configuration state from old cluster
$ tctl get all --with-secrets > state.yaml

# Prepare a new uninitialized backend (make sure to port
# any non-default config values from the old config file)
$ mkdir fresh && cat > fresh.yaml << EOF
teleport:
  data_dir: fresh
EOF

# bootstrap fresh server (kill the old one first!)
$ sudo teleport start --config fresh.yaml --bootstrap state.yaml

# from another terminal, verify state transferred correctly
$ tctl --config fresh.yaml get all
# <your state here>
```

The `--bootstrap` flag has no effect, except when the Auth Service initializes
its backend initialization on first startup, so it is safe for use in
supervised/High Availability contexts.

### Limitations

The `--bootstrap` flag doesn't re-trigger Trusted Cluster handshakes, so Trusted
Cluster resources need to be recreated manually.

All the same limitations around modifying the config file of an existing cluster
also apply to a new cluster being bootstrapped from the state of an old cluster:

  - Changing the cluster name will break your CAs. This will be caught and Teleport
    will refuse to start.
  - Some user authentication mechanisms (e.g. WebAuthn) require that the public
    endpoint of the Web UI remains the same. This cannot be caught by Teleport,
    so be careful!
  - Any Node whose invite token is defined in the Auth Service's configuration
    file will be able to join automatically, but Nodes that were added
    dynamically will need to be re-invited.

</TabItem>
<TabItem scope={["oss"]} label="Open Source">

As of version v4.1, you can now quickly export a collection of resources from
Teleport. This feature was designed to help customers migrate from local storage
to etcd.

Using `tctl get all --with-secrets` will retrieve the below items:

- Users
- Certificate Authorities
- Trusted Clusters
- GitHub Connectors
- Roles

When migrating backends, you should back up your Auth Service's
`data_dir/storage` directly.

### Example of backing up and restoring a cluster

```code
# Log in to your cluster with tsh so you can use tctl from your local machine.
# You can also run tctl on your Auth Service host without running "tsh login"
# first.
$ tsh login --user=myuser --proxy=teleport.example.com
# Export dynamic configuration state from old cluster
$ tctl get all --with-secrets > state.yaml

# Prepare a new uninitialized backend (make sure to port
# any non-default config values from the old config file)
$ mkdir fresh && cat > fresh.yaml << EOF
teleport:
  data_dir: fresh
EOF

# bootstrap fresh server (kill the old one first!)
$ sudo teleport start --config fresh.yaml --bootstrap state.yaml

# from another terminal, verify state transferred correctly
$ tctl --config fresh.yaml get all
# <your state here>
```

The `--bootstrap` flag has no effect, except when the Auth Service initializes
its backend initialization on first startup, so it is safe for use in
supervised/High Availability contexts.

### Limitations

The `--bootstrap` flag doesn't re-trigger Trusted Cluster handshakes, so Trusted
Cluster resources need to be recreated manually.

All the same limitations around modifying the config file of an existing cluster
also apply to a new cluster being bootstrapped from the state of an old cluster:

  - Changing the cluster name will break your CAs. This will be caught and Teleport
    will refuse to start.
  - Some user authentication mechanisms (e.g. WebAuthn) require that the public
    endpoint of the Web UI remains the same. This cannot be caught by Teleport,
    so be careful!
  - Any Node whose invite token is defined in the Auth Service's configuration
    file will be able to join automatically, but Nodes that were added
    dynamically will need to be re-invited.

</TabItem>
<TabItem scope={["cloud"]} label="Teleport Cloud">

In Teleport Cloud, backend data is managed for you automatically. 

If you would like to migrate configuration resources to a self-hosted Teleport
cluster, follow our recommended backup practice of storing configuration
resources in a git repository and running `tctl create -f` regularly for each
resource. 

This will enable you to keep your configuration resources up to date regardless
of storage backend.

</TabItem>
</Tabs>

### Active-passive Multi-region architecture example

For mission-critical Teleport use cases, it is recommended to use an active-passive 
deployment architecture that is highly available across multiple regions.
Active-passive architecture keeps Teleport accessible with minimal disruption during the event 
of a a cloud provider regional outage.  This example uses AWS, but the 
general architecture can be applied to various cloud providers and self-hosted infrastructures.

In this example, the Auth and Proxy cluster components, along with their networking components 
run parallell to each other in two different AWS regions. These seperate Teleport clusters share 
a storage backend for the  cluster state, audit logs, and share storage for session recordings. 
A DNS failover is the control point to switch between the active and passive clusters during the 
event of a regional outage. 

Key components of an Active-Passive Multi-region: 

- Two  Teleport clusters deployed in seperate regions. For example regions us-west-1 and us-east-1. 
- All Teleport cluster components deployed as autoscaling groups in both regions. 
- For this example the us-west-1 region Teleport cluster is the active cluster, and the us-east-1 
  region is the passive cluster. 
- DynamoDB for cluster state and audit log storage, and S3 buckets for session recording storage.
    - Enable the us-west-1 DynamoDB table with global replication in the us-east-1 passive region
      to ensure cluster state and audit logs are maintained in both regions.
    - Enable cross region replication on the S3 bucket to ensure objects are replicated between 
      both the active and passive regions.
- Configure the Teleport cluster Route53 records to use the [failover routing policy](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-passive) with the active cluster end 
  point as primary record and the passive cluster endpoint as the secondary record.
    - Using health checks to determine Teleport cluster availability, Route53 will automatically send the traffic to the passive 
      cluster during the event of a regional outage and automatically shifts the traffic back to the active 
      cluster when health checks report a normal state.
