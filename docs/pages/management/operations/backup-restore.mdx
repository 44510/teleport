---
title: Backup and Restore
description: How to back up and restore your Teleport cluster state.
---
This guide explains the components of your Teleport deployment that must be
backed up and lays out our recommended approach for performing backups.

(!docs/pages/includes/cloud/call-to-action.mdx!)

## What you should back up

### Teleport services
<Tabs>
<TabItem scope={["enterprise", "oss"]} label="Self-Hosted">

Teleport's Proxy Service and Nodes are stateless. For these components, only
`teleport.yaml` should be backed up.

The Auth Service is Teleport's brain, and depending on the backend should be
backed up regularly.

For example, a Teleport cluster running on AWS with DynamoDB must back up the
following data:

| What | Where ( Example AWS Customer ) |
| - | - |
| Local Users ( not SSO ) | DynamoDB |
| Certificate Authorities | DynamoDB |
| Trusted Clusters | DynamoDB |
| Connectors: SSO | DynamoDB / File System |
| RBAC | DynamoDB / File System |
| teleport.yaml | File System |
| teleport.service | File System |
| license.pem | File System |
| TLS key/certificate | File System / AWS Certificate Manager |
| Audit log | DynamoDB |
| Session recordings | S3 |

For this customer, we would recommend using [AWS best practices](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html) for backing up DynamoDB. If DynamoDB is used for
Teleport audit logs, logged events have a TTL of 1 year.

| Backend | Recommended backup strategy |
| - | - |
| Local Filesystem | Back up the `/var/lib/teleport/storage` directory and the output of `tctl get all --with-secrets`. |
| DynamoDB | [Follow AWS's guidelines for backup and restore](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html) |
| etcd | [Follow etcd's guidelines for disaster recovery](https://etcd.io/docs/v2/admin_guide) |
| Firestore | [Follow GCP's guidelines for automated backups](https://firebase.google.com/docs/database/backups) |

</TabItem>
<TabItem scope={["cloud"]} label="Teleport Cloud">

Teleport Cloud manages all Auth Service and Proxy Service backups.

While Teleport Nodes are stateless, you should ensure that you can restore their
configuration files.

</TabItem>
</Tabs>

### Teleport resources

Teleport uses YAML resources for roles, Trusted Clusters, local users, and authentication connectors.
These could be created via `tctl` or the Web UI.

You should back up your dynamic resource configurations to ensure that you can restore them in case of an outage.

## Our recommended backup practice

If you're running Teleport at scale, your teams need to have an automated way to restore Teleport. At a high level, this is our recommended approach:

<Tabs>
<TabItem scope={["enterprise", "oss"]} label="Self-Hosted">

- Persist and back up your backend.
- Share that backend among Auth Service instances.
- Store your dynamic resource configurations as discrete files in a git
  repository.
- Have your continuous integration system run `tctl create -f *.yaml` from the
  git repository. The `-f` flag instructs `tctl create` not to return an error
  if a resource already exists, so this command can be run regularly.

</TabItem>
<TabItem scope={["cloud"]} label="Teleport Cloud">

- Store your dynamic resource configurations as discrete files in a git
  repository.
- Have your continuous integration system run `tctl create -f *.yaml` from the
  git repository. The `-f` flag instructs `tctl create` not to return an error
  if a resource already exists, so this command can be run regularly.

</TabItem>
</Tabs>

## Migrating backends
<Tabs>
<TabItem scope={["enterprise"]} label="Teleport Enterprise">

Using `tctl get all` will retrieve the below items:

- Users
- Certificate Authorities
- Trusted Clusters
- Connectors:
  - GitHub
  - SAML
  - OIDC
- Roles

This command automatically includes all secrets in these resources. You
can also individually retrieve resources with `tctl get` (Example: `tctl get roles`).

When migrating backends, you should back up your Auth Service's
`data_dir/storage` directly.

### Example of backing up and restoring a cluster

```code
# Run this command on one of the Teleport Auth Service hosts
# Export dynamic configuration state from old cluster
$ tctl get all > state.yaml

# Prepare a new uninitialized backend (make sure to port
# any non-default config values from the old config file)
$ mkdir fresh && cat > fresh.yaml << EOF
teleport:
  data_dir: fresh
EOF

# bootstrap fresh server (kill the old one first!)
$ sudo teleport start --config fresh.yaml --bootstrap state.yaml

# from another terminal, verify state transferred correctly
$ tctl --config fresh.yaml get all
# <your state here>
```
<Notice>
The `tctl get all` requires using on the Auth service machine 
or existing User records will cause a permission error.  This is due
to User secrets only being available from a a direct `tctl` connection
on a Teleport Auth Service machine. As a workaround, retrieve resources
individually with the `--with-secrets` parameter except for users. Local user secrets
including passwords would need resetting after creating without secrets.

All trusted cluster certificate resources should be removed for use
in a bootstrap or you will receive an error.  These are the `kind: cert_authority`
records identifable with `cluster_name` that match to the leaf cluster names.
</Notice>

The `--bootstrap` flag has no effect, except when the Auth Service initializes
its backend initialization on first startup, so it is safe for use in
supervised/High Availability contexts.

### Limitations

The `--bootstrap` flag doesn't re-trigger Trusted Cluster handshakes, so Trusted
Cluster resources need to be recreated manually.

All the same limitations around modifying the config file of an existing cluster
also apply to a new cluster being bootstrapped from the state of an old cluster:

  - Changing the cluster name will break your CAs. This will be caught and Teleport
    will refuse to start.
  - Some user authentication mechanisms (e.g. WebAuthn) require that the public
    endpoint of the Web UI remains the same. This cannot be caught by Teleport,
    so be careful!
  - Any Node whose invite token is defined in the Auth Service's configuration
    file will be able to join automatically, but Nodes that were added
    dynamically will need to be re-invited.

</TabItem>
<TabItem scope={["oss"]} label="Open Source">

As of version v4.1, you can now quickly export a collection of resources from
Teleport. This feature was designed to help customers migrate from local storage
to etcd.

Using `tctl get all` will retrieve the below items:

- Users
- Certificate Authorities
- Trusted Clusters
- GitHub Connectors
- Roles

This command automatically includes all secrets in these resources. You
can also individually retrieve resources with `tctl get` (Example: `tctl get roles`).

When migrating backends, you should back up your Auth Service's
`data_dir/storage` directly.

### Example of backing up and restoring a cluster

```code
# Run this command on one of the Teleport Auth Service hosts
# Export dynamic configuration state from old cluster
$ tctl get all > state.yaml

# Prepare a new uninitialized backend (make sure to port
# any non-default config values from the old config file)
$ mkdir fresh && cat > fresh.yaml << EOF
teleport:
  data_dir: fresh
EOF

# bootstrap fresh server (kill the old one first!)
$ sudo teleport start --config fresh.yaml --bootstrap state.yaml

# from another terminal, verify state transferred correctly
$ tctl --config fresh.yaml get all
# <your state here>
```

<Notice>
The `tctl get all` requires using on the Auth service machine 
or existing User records will cause a permission error.  This is due
to User secrets only being available from a a direct `tctl` connection
on a Teleport Auth Service machine. As a workaround, retrieve resources
individually with the `--with-secrets` parameter except for users. Local user secrets
including passwords would need resetting after creating without secrets.

All trusted cluster certificate resources should be removed for use
in a bootstrap or you will receive an error.  These are the `kind: cert_authority`
records identifable with `cluster_name` that match to the leaf cluster names.
</Notice>

The `--bootstrap` flag has no effect, except when the Auth Service initializes
its backend initialization on first startup, so it is safe for use in
supervised/High Availability contexts.

### Limitations

The `--bootstrap` flag doesn't re-trigger Trusted Cluster handshakes, so Trusted
Cluster resources need to be recreated manually.

All the same limitations around modifying the config file of an existing cluster
also apply to a new cluster being bootstrapped from the state of an old cluster:

  - Changing the cluster name will break your CAs. This will be caught and Teleport
    will refuse to start.
  - Some user authentication mechanisms (e.g. WebAuthn) require that the public
    endpoint of the Web UI remains the same. This cannot be caught by Teleport,
    so be careful!
  - Any Node whose invite token is defined in the Auth Service's configuration
    file will be able to join automatically, but Nodes that were added
    dynamically will need to be re-invited.

</TabItem>
<TabItem scope={["cloud"]} label="Teleport Cloud">

In Teleport Cloud, backend data is managed for you automatically. 

If you would like to migrate configuration resources to a self-hosted Teleport
cluster, follow our recommended backup practice of storing configuration
resources in a git repository and running `tctl create -f` regularly for each
resource. 

This will enable you to keep your configuration resources up to date regardless
of storage backend.

</TabItem>
</Tabs>

